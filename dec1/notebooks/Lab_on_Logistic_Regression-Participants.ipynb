{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Intro to Data Science and Machine Learning with Python</h1> \n",
    "<h2 style=\"text-align: center;\" markdown=\"2\">Nov 30 - Dec 1, 2019</h2>\n",
    "<h3 style=\"text-align: center;\" markdown=\"3\">Lab on Logistic Regression</h3>\n",
    "\n",
    "\n",
    "> *This notebook is part of the Workshop on Introduction to Data Science and Machine Learning with Python, a 2-day workshop organized by NAAMII. The objectives of this notebook is to use Logistic Regression to predict whether an individual survived or not during the sinking of the Titanic. The data used in this exercise is adopted from https://www.kaggle.com/c/titanic/data.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Introduction](#introduction)    \n",
    "&nbsp;&nbsp;&nbsp;[Explanation and Theory](#introduction-theory)    \n",
    "&nbsp;&nbsp;&nbsp;[Considerations](#introduction-considerations)    \n",
    "[Data Setup](#setup)  \n",
    "&nbsp;&nbsp;&nbsp;[Meta data](#variables)  \n",
    "&nbsp;&nbsp;&nbsp;[Correlation](#correlation)  \n",
    "&nbsp;&nbsp;&nbsp;[Dummy variables for Categorical Data](#dummy)      \n",
    "[Scikit-Learn Logistic Regression](#sklearn-logreg)  \n",
    "[Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "Logistic regression is one of the most simple and well-known machine learning algorithms for classification. Despite its name, it is used for classification rather than regression. \n",
    "\n",
    "In basic terms, it predicts the probability of occurrence of an event by fitting the data to a logistic function. This probability is then translated into a class label based on the set threshold of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation and Theory <a class=\"anchor\" id=\"introduction-theory\"></a>\n",
    "\n",
    "**Assumptions and properties.** \n",
    "Suppose we have a data set that consists of n samples and m features\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:samples}\n",
    "\\mathbf{X} = \\{\\mathbf{x_1}, \\mathbf{x_2}, \\ldots \\mathbf{x_n}\\}, \\quad   \\mathbf{x_i} \\in \\mathbb{R}^{m}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:features}\n",
    "\\mathbf{x_i} =  (x_i^{(1)}, x_i^{(2)}, \\ldots x_i^{(m)} ) ^T\n",
    "\\end{equation}\n",
    "\n",
    "The target variable is the probability of a sample belonging to a certain class and is represented by\n",
    "\\begin{equation}\n",
    "\\label{eqn:target}\n",
    "\\mathbf{Y}= \\{y_1, y_2 \\ldots y_n \\} \\quad where \\,\\,\\, y_i \\in (0,1)\n",
    "\\end{equation}\n",
    "\n",
    "Let us assume our problem is a binary classification problem, meaning the response/dependent variables has two classes or labels 0 and 1. If we used linear regression, it would give us a straight line that best separates 0 and 1 responses. However, we could not use this line to give us a probability, since it would give us a negative value for the responses near zero on the x-axis. Instead, when we use logistic regression, we fit this data and estimate the target variable using the following **logistic/sigmoid function**:  \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eqn:sigmoid}\n",
    "y_i =\\frac {e^{(\\beta_0 + \\mathbf{\\beta} \\cdot \\mathbf{x_i})}} {(1 + e^{(\\beta_0 + \\mathbf{\\beta} \\cdot \\mathbf{x_i})})}\n",
    "\\end{equation}\n",
    "where $\\beta_0$ is called the bias term or the intercept, and $\\mathbf{\\beta}$ are the coefficients associated with the feature vector $\\mathbf{x_i}$. \n",
    "<img alt=\"logistic function diagram\" src=\"logit.png\"/>\n",
    "\n",
    "The function transforms all input variables to the range [0,1], which brings the smallest or most negative numbers close to zero and the largest positive numbers close to one. This allows us to take real-valued inputs and output a probability of the input belonging to either class zero or one. We can then choose a threshold value, such as 0.5, and provide the class output.\n",
    "\n",
    "**Algorithm and Training.** Logistic regression takes the form of a linear model:\n",
    "\n",
    "$$f(i)=\\beta_0+\\beta_1x_{1,i}+...+\\beta_mx_{m,i} $$\n",
    "\n",
    "where $\\beta_0,...,\\beta_m$ are the regression coefficients or weights assigned to each feature $x$. For each data point $i$, a pseudo-variable $x_{0,i}=1$ is added to correspond to the intercept coefficient $\\beta_0$. This allows us to write the model in vector form as:\n",
    "$$f(i)=\\boldsymbol{\\beta}\\cdot\\boldsymbol{X_i}$$\n",
    "\n",
    "When we train the logistic regression classifier, we are trying to find the best values of $\\beta$ to match the data. This is done using an estimation method that attempts to minimize the error of the model. There are several techniques to do this, such as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations <a class=\"anchor\" id=\"introduction-considerations\"></a>\n",
    "\n",
    "There are a few things to remember when using logistic regression as a classifier. First, it assumes that there is a linear relationship between the independent variables and the dependent variables. In high-dimensional datasets, this may not be the case, so logistic regression may not be the best choice of classifier.\n",
    "\n",
    "Logisitic regression is also sensitive to highly correlated inputs. Having highly correlated inputs can cause the model to be overfit or will cause the model to fail to converge. We will take a closer look at the correlations between different variables in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vSzTxrDg46Vv"
   },
   "source": [
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/1280px-RMS_Titanic_3.jpg)\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "Our goal is to use predict if an individual survived or not in the titanic ship wreck. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ban1LsHV47fI"
   },
   "outputs": [],
   "source": [
    "# All our imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNw6JofC4_Nf"
   },
   "outputs": [],
   "source": [
    "#Run this only if you are using Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfzC3gW64_3e"
   },
   "outputs": [],
   "source": [
    "#Enter the path of your file inside the quotes\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWaSW9Tv5Q5W"
   },
   "outputs": [],
   "source": [
    "#Write the code to read the csv file to a dataframe df\n",
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qyou3IbK5TQu"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XLD5-L35jyb"
   },
   "source": [
    "# Variable Metadata <a class=\"anchor\" id=\"variables\"></a>\n",
    "**Pclass:** A proxy for socio-economic status (SES)\n",
    "\n",
    "1 = Upper\n",
    "\n",
    "2 = Middle\n",
    "\n",
    "3 = Lower\n",
    "\n",
    "**Age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "**SibSp:** The dataset defines family relations in this way:\n",
    "\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "\n",
    "Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "\n",
    "\n",
    "**Parch:** The dataset defines family relations in this way:\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n",
    "**Embarked:** The port from where the particular passenger was embarked/boarded. \n",
    "\n",
    "**Survived:** \n",
    "0: if the person did not survive\n",
    "1: if the person survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "iyGlMv-O5L6e",
    "outputId": "deac0d7c-e574-4d9d-ebf0-74b3986c4ef0"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHy_Po1H-YNT"
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between variables<a class=\"anchor\" id=\"correlation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
    "       'Ticket', 'Fare', 'Embarked', 'Survived', 'Initial']\n",
    "# Calculate the correlations\n",
    "corr_mat = df[variables].corr().round(2)\n",
    "\n",
    "# Draw a correlation heatmap\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n",
    "            cmap = plt.cm.RdYlGn_r, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us look at the dataset. What columns do you think are important variables to detect Survival? Does Name matter? Or, does Ticket number matter? What about Pclass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peJBQQPvL2YY"
   },
   "outputs": [],
   "source": [
    "df[df['Survived']==1].Age.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-c3eYmyGL6bV"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Sex', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyIgZPOvL-Mm"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Embarked', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIxzbAkxMLJl"
   },
   "outputs": [],
   "source": [
    "sns.countplot('Embarked',hue='Survived',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iU8EF-eTMM_O"
   },
   "outputs": [],
   "source": [
    "df.groupby(['Pclass', 'Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaYoxpSN11sV"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"SibSp\", y=\"Survived\", data=df,  kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZnO6iLFrq_P"
   },
   "source": [
    "# Dummy variables for categorical data <a class=\"anchor\" id=\"dummy\"></a>\n",
    "Sex and Embarked are categorical data. We will eventually be using Scikit-learn to create our ML model which expects numeric data. We will have to convert categorical variable into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BRX_39P3tbJl",
    "outputId": "c466d0c5-1ac7-4b17-fc99-8d741eac6aa2"
   },
   "outputs": [],
   "source": [
    "# Try this, what do you see\n",
    "pd.get_dummies(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQbW63S16Qu2"
   },
   "outputs": [],
   "source": [
    "# Let us create dummy variables for \"Sex\" column\n",
    "# As you might have noticed, instead of storing male as 0 and 1,\n",
    "# and females as 0 and 1, storing one of them will suffice as they\n",
    "# are mutually exclusive in our dataset. Therefore, we use the drop_first argument\n",
    "\n",
    "sex = pd.get_dummies(df['Sex'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5EOQ2h16ZZ-"
   },
   "outputs": [],
   "source": [
    "# Lets do the same with our Embarked variable, what do you think will happen\n",
    "# when three variables are converted to dummy variables with drop_first argument?\n",
    "#We do this to avoid something called the \"dummy variable trap\". \n",
    "\n",
    "#Write the code to create dummy variables for \"Embarked\"\n",
    "embarked = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5CKONVA6w8t"
   },
   "outputs": [],
   "source": [
    "# Removing Categorical Columns and Columns we decided not to use \n",
    "df.drop(['Name', 'Sex', 'Ticket', 'Embarked'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "L-_Hjvl6-BFE",
    "outputId": "8c351b5b-597c-4e19-9670-6a36b40ec40a"
   },
   "outputs": [],
   "source": [
    "# what does the dataset look like now\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkJOo61n-G8M"
   },
   "outputs": [],
   "source": [
    "# Lets add our dummy/indicator columns back to the dataset\n",
    "df = pd.concat([df, sex, embarked], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "xFE1CPFw-Stb",
    "outputId": "07e3fc8d-34ba-4150-b0b0-69d3350d8ae2"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWT9ui6l9XmK"
   },
   "source": [
    "Also, PassengerID is simply used for identification purposes, and Survived holds the \"label\" data for our result. We can get rid of these columns. The Survived column is the data we feed the machine learning algorithms to learn from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46Ppu76i9_sZ"
   },
   "outputs": [],
   "source": [
    "traning_data_set = df.drop(['PassengerId', 'Initial','Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUs4c3Rw-U60"
   },
   "outputs": [],
   "source": [
    "# Let us split the data in training and testing set. The test_size parameter allows you to control the ratio of \n",
    "# testing set you want to randomly capture from your data set. \n",
    "# With the random_state parameter you can guarantee that the output of Run 1 will be equal to the output of Run 2, \n",
    "# i.e. your split will be always the same.\n",
    "\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "velHrfoUAKhL",
    "outputId": "4416a7c9-a4b2-4d7c-8537-524eae80e938"
   },
   "outputs": [],
   "source": [
    "# Let us look at the shape our training and testing splits\n",
    "X_train.shape, X_test.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_6lV7EQAZHz"
   },
   "outputs": [],
   "source": [
    "# We will be using Logistic Regression for our purposes.\n",
    "# If you look at the documentation, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# There are several additional parameters you can pass. For our purposes we will be using the defaults.\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "ZpWb4XdmBwIt",
    "outputId": "3b97233d-0640-4154-c6cc-e5b4d447ed63"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "anixVhJ0Bx4m",
    "outputId": "3c393fe6-054e-476d-84c8-6e8a8862a254"
   },
   "outputs": [],
   "source": [
    "# Let us fit our training data\n",
    "# This is where the learning is happening\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NWpwb5QMBxdx"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "DIGyYEVsCDjS",
    "outputId": "bdf9da3b-7163-4f73-8e40-af544130fde5"
   },
   "outputs": [],
   "source": [
    "# What did we predict?\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGpkTSobAj2b"
   },
   "outputs": [],
   "source": [
    "# What did our test dataset look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIlp_KLNASj3"
   },
   "outputs": [],
   "source": [
    "df_compare = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "xITI44wyOEzj",
    "outputId": "1079056d-1442-4694-c4b0-55ded438aaf3"
   },
   "outputs": [],
   "source": [
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AvyL3elOIXS"
   },
   "outputs": [],
   "source": [
    "#Adding columns 'Survived' and 'Predicted' to df_compare.\n",
    "df_compare['Survived'] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLjh6NuyORIu"
   },
   "outputs": [],
   "source": [
    "df_compare['Predicted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "kYcz9HmYOX3u",
    "outputId": "de032910-6a79-48f3-accf-fcae72336594"
   },
   "outputs": [],
   "source": [
    "df_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GuphspQJFzQL",
    "outputId": "322e8f46-4529-4ff5-a883-d7cd027b4acc"
   },
   "outputs": [],
   "source": [
    "# We can now check the accuracy of the model.\n",
    "df_compare.loc[df_compare.Survived==df_compare.Predicted].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lu1T84spGLGR",
    "outputId": "36de079f-b139-4daf-befb-8e3c43dbff1b"
   },
   "outputs": [],
   "source": [
    "df_compare.loc[df_compare.Survived==df_compare.Predicted].shape[0]/df_compare.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can also call the score function to calculate the accuracy of the algorithm\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EoJbhnYKQmes",
    "outputId": "5decba89-71e9-4398-d258-7614fff17479"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VY_oy4qmSmDH"
   },
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/en/a/a6/Binary_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iShFpzbEVQ7V"
   },
   "source": [
    "![alt text](https://github.com/ayushsubedi/naivebayes_spirathon2019/raw/e43807530ec059ec26c05dc1f646cb875fab514c/types.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TibcRw-WATB"
   },
   "source": [
    "In our context,\n",
    "\n",
    "\n",
    "*   True Positive: (Good) People who really survived were predicted to survive.\n",
    "*   True Negative: (Good) People who really did not survive were predicted to not survive. \n",
    "*   False Positive: (Bad) People who really did not survive were predicted to survive.\n",
    "*   False Negative: (Bad) People who really survived were predicted to not survive. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X54LAxE6UmSF"
   },
   "source": [
    "You can also calculate accuracy by (TP+TN)/(TP+FN+FP+TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbOvOiu7Ta-V"
   },
   "source": [
    "But wait, is accuracy a good indication of how good/bad a model is? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu85Qv6qXfo2"
   },
   "source": [
    "There are 284335 good credit card transactions and 472 fradulent transactions. Can you come up with a model that has over 99% accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htcAZr3MYFS8"
   },
   "outputs": [],
   "source": [
    "total_transactions = 284335 + 472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYC9TYKYYXOt"
   },
   "outputs": [],
   "source": [
    "# Let us say we wrote a bad model which predicts everything to be not fradulent\n",
    "not_fraud = total_transactions\n",
    "correct_prediction = not_fraud - 472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nle2eiZYPvD"
   },
   "outputs": [],
   "source": [
    "accuracy = correct_prediction / total_transactions * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0WdU6HzxY7VZ",
    "outputId": "0d09f33f-1e2d-4e97-c065-6ef209c66fc9"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81uc4SUcY5IZ"
   },
   "source": [
    "The accuracy of the model is 99%. But what is the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RE9yn10Xj-X"
   },
   "source": [
    "Since evaluation of a model based on the accuracy metric does not always provide how good or bad of a model we have built, there are other metrics that can be used:\n",
    "  Learn more here: https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Summary <a class=\"anchor\" id=\"summary\"></a>\n",
    "In this notebook, we have covered the basic concepts of a logistic regression classifier, and applied it to the titanic dataset from Kaggle to predict if an individual survived the wreck or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 5. Introduction to Machine Learning Logistic Regression",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
